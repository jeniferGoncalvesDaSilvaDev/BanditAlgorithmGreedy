# ğŸ° Algoritmo Îµ-Greedy Interativo

Uma aplicaÃ§Ã£o web interativa que demonstra o algoritmo Îµ-Greedy, uma estratÃ©gia fundamental de aprendizado por reforÃ§o usado em problemas de Multi-Armed Bandit.

## ğŸ“‹ Sobre o Projeto

Este projeto implementa uma simulaÃ§Ã£o visual do algoritmo Îµ-Greedy, permitindo que usuÃ¡rios experimentem com diferentes parÃ¢metros e vejam em tempo real como o algoritmo equilibra **exploraÃ§Ã£o** (testar novas opÃ§Ãµes) e **explotaÃ§Ã£o** (usar a melhor opÃ§Ã£o conhecida).

### ğŸ¯ O Problema Multi-Armed Bandit

Imagine que vocÃª estÃ¡ em um cassino com vÃ¡rias mÃ¡quinas caÃ§a-nÃ­queis (bandits), cada uma com uma probabilidade diferente de dar prÃªmio. Seu objetivo Ã© maximizar seus ganhos, mas vocÃª nÃ£o sabe qual mÃ¡quina Ã© a melhor. VocÃª precisa:

- **Explorar**: Testar diferentes mÃ¡quinas para descobrir qual Ã© melhor
- **Explotar**: Usar a mÃ¡quina que parece ser a melhor atÃ© agora

O algoritmo Îµ-Greedy resolve este dilema usando uma estratÃ©gia simples mas eficaz.

## ğŸš€ Como Usar

1. **Acesse a aplicaÃ§Ã£o**: Abra o arquivo `index.html` em um navegador moderno
2. **Ajuste os parÃ¢metros**:
   - **k**: NÃºmero de alavancas/opÃ§Ãµes (2-50)
   - **T**: NÃºmero de tentativas (100-5000)
   - **Îµ inicial**: Probabilidade de exploraÃ§Ã£o (0.1-10.0)
3. **Execute**: Clique em "ğŸš€ Executar SimulaÃ§Ã£o"
4. **Analise**: Observe os grÃ¡ficos de regret e aÃ§Ãµes Ã³timas

## ğŸ“Š Interpretando os Resultados

### GrÃ¡fico 1: Regret Acumulado
- **O que mostra**: O "arrependimento" mÃ©dio por nÃ£o escolher sempre a melhor opÃ§Ã£o
- **TendÃªncia ideal**: Deve diminuir ao longo do tempo
- **InterpretaÃ§Ã£o**: Valores menores = algoritmo estÃ¡ aprendendo melhor

### GrÃ¡fico 2: AÃ§Ãµes Ã“timas (%)
- **O que mostra**: Porcentagem de vezes que o algoritmo escolheu a melhor opÃ§Ã£o
- **TendÃªncia ideal**: Deve aumentar ao longo do tempo
- **InterpretaÃ§Ã£o**: Valores maiores = algoritmo estÃ¡ melhorando suas decisÃµes

## ğŸ”¬ Como Funciona o Algoritmo Îµ-Greedy

```python
# PseudocÃ³digo simplificado
for cada tentativa t:
    if random() < Îµ:
        # EXPLORAÃ‡ÃƒO: escolha uma alavanca aleatÃ³ria
        alavanca = escolha_aleatoria()
    else:
        # EXPLOTAÃ‡ÃƒO: escolha a melhor alavanca conhecida
        alavanca = melhor_alavanca_atual()
    
    recompensa = puxar_alavanca(alavanca)
    atualizar_estimativas(alavanca, recompensa)
    
    # Reduzir Îµ ao longo do tempo (mais explotaÃ§Ã£o)
    Îµ = Îµ * fator_decaimento
```

### ParÃ¢metros Principais

- **Îµ (epsilon)**: Controla o equilÃ­brio exploraÃ§Ã£o/explotaÃ§Ã£o
  - Îµ alto = mais exploraÃ§Ã£o (arriscado, mas pode encontrar melhores opÃ§Ãµes)
  - Îµ baixo = mais explotaÃ§Ã£o (conservador, usa conhecimento atual)
  
- **Decaimento de Îµ**: O valor de Îµ diminui ao longo do tempo
  - InÃ­cio: Mais exploraÃ§Ã£o para descobrir as opÃ§Ãµes
  - Final: Mais explotaÃ§Ã£o para maximizar ganhos

## ğŸ› ï¸ Tecnologias Utilizadas

- **HTML5**: Estrutura da interface
- **CSS3**: EstilizaÃ§Ã£o responsiva e moderna
- **PyScript**: ExecuÃ§Ã£o de Python no navegador
- **Matplotlib**: GeraÃ§Ã£o de grÃ¡ficos interativos
- **NumPy**: CÃ¡lculos matemÃ¡ticos e estatÃ­sticos

## ğŸ“ Estrutura do Projeto

```
â”œâ”€â”€ index.html          # AplicaÃ§Ã£o principal
â”œâ”€â”€ README.md           # Este arquivo
â””â”€â”€ replit.md          # DocumentaÃ§Ã£o tÃ©cnica do projeto
```

## ğŸ® Experimentos Sugeridos

### Experimento 1: Efeito do Îµ inicial
- Execute com Îµ = 0.1, 1.0, e 5.0
- Observe como valores diferentes afetam a velocidade de aprendizado

### Experimento 2: NÃºmero de alavancas
- Compare k = 5 vs k = 20
- Veja como mais opÃ§Ãµes tornam o problema mais difÃ­cil

### Experimento 3: DuraÃ§Ã£o da simulaÃ§Ã£o
- Execute com T = 500 vs T = 2000
- Observe como mais tentativas melhoram o desempenho

## ğŸ“š Conceitos de Aprendizado por ReforÃ§o

Este projeto demonstra conceitos fundamentais:

- **Multi-Armed Bandit**: Problema clÃ¡ssico de tomada de decisÃ£o
- **Exploration vs Exploitation**: Dilema fundamental em IA
- **Regret**: MÃ©trica para avaliar qualidade das decisÃµes
- **Îµ-Greedy Strategy**: Algoritmo simples mas eficaz
- **ConvergÃªncia**: Como algoritmos melhoram ao longo do tempo

## ğŸ”„ Como Executar Localmente

1. Clone ou baixe o projeto
2. Abra `index.html` em um navegador moderno
3. Aguarde o PyScript carregar (pode levar alguns segundos)
4. Comece a experimentar!

**Requisitos**: Navegador moderno com suporte a WebAssembly (Chrome, Firefox, Safari, Edge)

## ğŸ“ AplicaÃ§Ãµes PrÃ¡ticas

O algoritmo Îµ-Greedy Ã© usado em:

- **RecomendaÃ§Ãµes online**: Testar novos produtos vs recomendar favoritos
- **Testes A/B**: Equilibrar coleta de dados com performance
- **Publicidade**: Escolher anÃºncios para maximizar cliques
- **Games**: NPCs que aprendem estratÃ©gias
- **RobÃ³tica**: Explorar ambiente vs usar conhecimento atual

## ğŸ“ˆ MÃ©tricas de Performance

A aplicaÃ§Ã£o calcula automaticamente:

- **Regret mÃ©dio final**: Qualidade geral das decisÃµes
- **% de aÃ§Ãµes Ã³timas final**: EficÃ¡cia do aprendizado
- **ConvergÃªncia visual**: AtravÃ©s dos grÃ¡ficos ao longo do tempo

## ğŸ¤ CrÃ©ditos

Baseado no trabalho de: https://github.com/petroud/E-greedy_and_UCB_algorithms/

## ğŸ“„ LicenÃ§a

Este projeto Ã© educacional e estÃ¡ disponÃ­vel para uso livre em contextos acadÃªmicos e de aprendizado.

---

**Desenvolvido para ensinar conceitos de Aprendizado por ReforÃ§o de forma visual e interativa** ğŸ¯